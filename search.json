[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Anatoliy’s Blog"
  },
  {
    "objectID": "blog/2016/11/10/spark-shell-without-spark/index.html",
    "href": "blog/2016/11/10/spark-shell-without-spark/index.html",
    "title": "spark-shell without Spark",
    "section": "",
    "text": "spark-shell is an interactive shell that comes with spark’s distribution. Shell is useful for learning api, quick experiments, prototyping and etc. But to do that you don’t need to have cluster or even have spark distribution installed.\n\nWhat you need to have is a basic knowledge of scala and sbt. Just create build.sbt file with following content:\n\n\nbuild.sbt\n\nscalaVersion := \"2.11.8\"\n\nval sparkVersion = \"2.0.1\"\n\nlibraryDependencies ++= Seq(\n  \"org.apache.spark\" %% \"spark-core\" % sparkVersion,\n  \"org.apache.spark\" %% \"spark-sql\" % sparkVersion\n)\n\nIt defines two spark components as dependencies: core and sql, we will use sql later for testing. You can import other spark libraries that you need like MLLib, Streaming and etc.\nAnd now you’re all set to start experimenting with Spark.\nFirst, run\n$ sbt console\nThis command starts the Scala interpreter REPL with specified dependencies on the classpath. So you can use Spark classes in the REPL.\nLet’s start with setting up a SparkSession.\nimport org.apache.spark.sql.SparkSession\n\nval spark = SparkSession.\n  builder().\n  appName(\"Console Demo\").\n  master(\"local[*]\").\n  getOrCreate()\nSparkSession is a new class in Spark 2.0 that gives you unified access to Spark SQL functionality.\nThe trick here is to set master URL to “local” to run Spark locally, local[*] means that spark will use as many worker threads as number of logical cores on your machine.\nAnd now you can run some spark code.\nval events = spark.sparkContext.parallelize(\n  \"\"\"{\"action\":\"create\",\"timestamp\":\"2016-11-06T00:01:17Z\"}\"\"\" :: Nil)\n\nval df = spark.read.json(events)\n\ndf.show()\nBy default you should see tons of Spark logs in console, but in the end you will see the result:\n+------+--------------------+\n|action|           timestamp|\n+------+--------------------+\n|create|2016-01-07T00:01:17Z|\n+------+--------------------+\nIn case if you don’t need SparkSQL functionality, you can create just SparkContext with local master.\nimport org.apache.spark.{SparkContext, SparkConf}\n\nval conf = new SparkConf().\n  setAppName(\"Console Demo\").\n  setMaster(\"local[*]\")\n\nval sc = new SparkContext(conf)\nSome spark features might also require setting additional spark options, but it depends on a feature.\nAnd that’s all what you need to do to run spark locally within spark console.\nHave fun playing with spark in console!"
  },
  {
    "objectID": "blog/2016/04/29/when-data-driven-app-smells-bad/index.html",
    "href": "blog/2016/04/29/when-data-driven-app-smells-bad/index.html",
    "title": "When Data Driven App Smells Bad",
    "section": "",
    "text": "Recently I have been involved in reviewing one “Big Data” project. I can’t say that volume of data processed by that app was that big, but the project was treated as real big data project by people who developed it. They used Hadoop just because that is what you use for big data. And even project’s codename had phrase “Big Data” in it.\nIn short, that project was a data analytics application with ETL, data warehouse and reporting. A variety of technologies were used to build it: Java, Hadoop, Python, Pandas, R with UI implemented in PHP and Shiny Apps embedded as IFRAMEs. But honestly that project was a real mess. And everything that could go wrong actually went wrong. Yet it is a good example of common problems and anti-patterns that might happen with data analytics apps. So I’d like to talk about major ones."
  },
  {
    "objectID": "blog/2016/04/29/when-data-driven-app-smells-bad/index.html#naive-productionizing",
    "href": "blog/2016/04/29/when-data-driven-app-smells-bad/index.html#naive-productionizing",
    "title": "When Data Driven App Smells Bad",
    "section": "Naive Productionizing",
    "text": "Naive Productionizing\nThere is nothing more permanent than prototypes. Code written as a demo or as a part of research becomes a part of a production system and entire system is built around that prototype code.\nIt might happen as following. Data scientist starts research using a dump of data, usually in a CSV format, and as a result he/she has a few scripts, it might be Jupyter notebooks written with python, R or etc. Then those notebooks are transformed into a bunch of standalone scripts with a few tweaks, so that scripts can interact with other parts of the system. But surprisingly code that worked well with sample data on a data scientist’s laptop doesn’t work with real data. So developers start to add more workarounds just to make those scripts working in a real production environment introducing more mess."
  },
  {
    "objectID": "blog/2016/04/29/when-data-driven-app-smells-bad/index.html#csv-driven-architecture",
    "href": "blog/2016/04/29/when-data-driven-app-smells-bad/index.html#csv-driven-architecture",
    "title": "When Data Driven App Smells Bad",
    "section": "CSV Driven Architecture",
    "text": "CSV Driven Architecture\nYou can find such architecture when somebody tries to glue multiple apps or services together passing data between them in CSV files. For example it can be multi step data analysis process, where each step is a separate script that gets and returns CSV data. There is a very big chance to fall in this if you are building system around existing notebooks, as mentioned in the previous section.\nThere is nothing wrong with CSV format, it is good for development, research, data exploration or even as interchange format with external systems. But using CSV or any other text format (like JSON and etc.) as a storage for intermediate results is not efficient. CPU time is wasted during serialization/deserialization on writes/reads, not mentioning that text files take more space.\nIn general you should avoid unnecessary data transformations. Try to do all steps in one job, so you don’t need any intermediate storage. If you need to reuse result of some step, prefer caching it in memory, Spark’s RDD and tables caching are good examples. Another option is to load results into database in a temporary table and query it."
  },
  {
    "objectID": "blog/2016/04/29/when-data-driven-app-smells-bad/index.html#overlooking-database-solutions",
    "href": "blog/2016/04/29/when-data-driven-app-smells-bad/index.html#overlooking-database-solutions",
    "title": "When Data Driven App Smells Bad",
    "section": "Overlooking Database solutions",
    "text": "Overlooking Database solutions\nRather than develop queries on top of database, the solution is to read raw data, e.g. logs, and process records one-by-one without using database. There is even more extreme case exists, when database is used for storage, but raw data is unloaded to CSV or is read using DB cursor in memory before being processed. There are no excuses not to use SQL queries, if your data is not huge so relational databases can be used. Databases are designed to efficiently execute queries, so in most cases it has better performance than custom-written solution. Same also applies to NoSQL solutions.\nSo try to design you system to write data into database and run analytical queries there. If the amount of data is big you still have options like MemSQL, or you can use columnar oriented storage like parquet or ORC File with distributed query execution engines like Presto or Spark. Of course there are cases when specific analysis required that’s might be hard to implement in query, but at least try to get as much as possible from storage layer. Push down filtering, aggregation and etc. to database and then process resulted smaller dataset with custom logic."
  },
  {
    "objectID": "blog/2016/04/29/when-data-driven-app-smells-bad/index.html#non-incremental-queries",
    "href": "blog/2016/04/29/when-data-driven-app-smells-bad/index.html#non-incremental-queries",
    "title": "When Data Driven App Smells Bad",
    "section": "Non incremental queries",
    "text": "Non incremental queries\nIn general processing same data all over again is a bad smell. Suppose you need a per-month report, so you get all historical data and aggregate it by month. But in order to keep it up to date each month you regenerate entire report using all historical data. This solution is simple and yet is not efficient: it wastes resources crunching all historical data all over again. Making this query incremental is more efficient solution. In this case just add aggregated data for a new month to previously calculated result.\nExample above demonstrates very simple case, although in more complex cases like dashboards, it is possible to make incremental queries by updating previous result."
  },
  {
    "objectID": "blog/2016/04/29/when-data-driven-app-smells-bad/index.html#improper-scaling",
    "href": "blog/2016/04/29/when-data-driven-app-smells-bad/index.html#improper-scaling",
    "title": "When Data Driven App Smells Bad",
    "section": "Improper scaling",
    "text": "Improper scaling\nImagine you have a working app that processes data it might be a java app, python script, etc. And you are pretty satisfied with its performance. But one day app begins to crash with out of memory error. Suddenly the volume of data has increased, maybe due to natural growth, or you started to use the app with real data in production instead of testing environment. So you decide to simply use more powerful instance to run this app. Now days it’s so easy to do using cloud providers. But there are might be a couple of problems with such fix. First, this is not a long term solution, since amount of data volume might continue to increase and you will have to upgrade again and again. Second, if you simply used more powerful instance, chances are that you also got more CPU cores, which will do nothing if your app is single threaded, which is a case for python/pandas. So you will simply waste money on unused CPU resources.\nThe opposite is also true. E.g. Hadoop is used when much simpler solutions are suitable. Those posts are good examples of this point.\nI don’t say that scaling up is worse than scaling out, the point is you must be aware of tradeoffs, limitation and consequences of each option. Be sure that you do not waste resources. Use solutions that support scaling out, or use cluster managers like Mesos for better resources utilization."
  },
  {
    "objectID": "blog/2016/04/29/when-data-driven-app-smells-bad/index.html#reinventing-the-wheel",
    "href": "blog/2016/04/29/when-data-driven-app-smells-bad/index.html#reinventing-the-wheel",
    "title": "When Data Driven App Smells Bad",
    "section": "Reinventing the wheel",
    "text": "Reinventing the wheel\nThere was a Hadoop job that generated CSV output in a project I mentioned in the beginning. And CSV generation was implemented by simply joining fields using comma delimiter. If you think that writing CSV is simple as concatenation of fields using comma, and parsing CSV is simple as splitting string by comma, then you are wrong. CSV is not only about commas. So some rows in resulted CSV were malformed, making them not processable by other tools. The worst thing here is that script on a next step just skipped malformed rows without failing. And missing data from bad rows cannot be treated as random noise in data.\nSo the lesson is never ever write your own data serialization library for CSV, JSON or any other format, many languages and frameworks has well-developed libraries for this. It is always a bad idea to reinvent the wheel, especially for such common tasks, like writing/reading CSV. But this anti-pattern is not only about data formats. It is also applies to integrating with third party services and etc."
  },
  {
    "objectID": "blog/2016/04/29/when-data-driven-app-smells-bad/index.html#takeaways",
    "href": "blog/2016/04/29/when-data-driven-app-smells-bad/index.html#takeaways",
    "title": "When Data Driven App Smells Bad",
    "section": "Takeaways",
    "text": "Takeaways\nI don’t aim to hate any particular technology like Hadoop, pandas or etc., you need to know your options.\nAs mentioned in the beginning it all starts with wrong attempt to reuse research or prototype code in production. What could have been done instead? Yes, prototypes are important. And based on its results developers need to figure out what real business needs are. Since not all reports that data scientists came up with might be useful. Then understand what really happens in those prototype apps:\n\nWhat is input data and what is an output?\nWhat are business requirements?\nIs it real time reports or not?\nAsk how data transformations can be simplified: combining or even removing some intermediate steps?\nUnderstand what is better data storage, database, plain files, etc.\nUnderstand data flows, can all or some calculations be pushed down to the storage layer.\n\nAnd last but not least, do not forget to try out multiple options, experiment with different approaches and technologies, and run benchmarks.\nThat’s all that I have at this moment. Surely there are much more anti-patters in data processing area. Have you seen any of those anti-patterns? I would love to hear your stories and thoughts on this topic."
  },
  {
    "objectID": "blog/2015/11/13/running-spark-shell-in-browser-with-apache-mesos-and-marathon/index.html",
    "href": "blog/2015/11/13/running-spark-shell-in-browser-with-apache-mesos-and-marathon/index.html",
    "title": "Running spark-shell in browser with Apache Mesos and Marathon",
    "section": "",
    "text": "I’d like to share a small trick on how to run spark-shell as a web app using Mesos and Marathon framework.\nThis might be useful for debugging or just to try some spark code. And to do that you don’t need to install spark on a cluster.\n\nAll you need are Mesos cluster, Marathon framework running on top of Mesos, and Java runtime installed on Mesos slaves.\n\nSorry guys, I’m not going to discuss here how to setup Mesos cluster, that would be another story. Here I just assume that you already have running environment with mentioned components.\n\nAnother secret ingredient is a gotty tool, it simply wraps terminal session into a web app. So the idea is to run spark-shell with gotty which is run by Marathon. Sounds simple? Here how it can be done.\nLet’s create a marathon app definition in a spark-shell.json file. You can read more about it here.\n\n\nspark-shell.json\n\n{\n  \"id\": \"/spark-shell\",\n  \"cmd\": \"./gotty -w -p $PORT0 --title-format 'Spark Shell' spark-1.5.1-bin-hadoop2.6/bin/spark-shell\",\n  \"cpus\": 0.2,\n  \"mem\": 256,\n  \"ports\": [\n    0\n  ],\n  \"instances\": 1,\n  \"env\": {\n    \"MESOS_NATIVE_JAVA_LIBRARY\": \"/usr/lib/libmesos.so\",\n    \"SPARK_EXECUTOR_URI\": \"http://d3kbcqa49mib13.cloudfront.net/spark-1.5.1-bin-hadoop2.6.tgz\",\n    \"MASTER\": \"mesos://zk://zk.cluster:2181/mesos\"\n  },\n  \"uris\": [\n    \"http://d3kbcqa49mib13.cloudfront.net/spark-1.5.1-bin-hadoop2.6.tgz\",\n    \"https://github.com/yudai/gotty/releases/download/v0.0.12/gotty_linux_amd64.tar.gz\"\n  ]\n}\n\nHere you might need to update MESOS_NATIVE_JAVA_LIBRARY, this value depends on your system and installation. MASTER should point to Mesos master. You might also want to change port value if you use any kind of service discovery with Marathon.\ngotty is used with following keys:\n\n-w allows writes to a terminal session, so you can interact with shell.\n-p $PORT0 sets the port gotty listens to, $PORT0 variable is assigned by Marathon.\n\nYou can submit app to Marathon using your preferred http client tool, e.g. for httpie just run\nhttp POST http://marathon.cluster/v2/apps &lt; spark-shell.json\nAfter that you’ll be able to see the new app in Marathon Web UI and its url once it’s started. When you visit that URL you should see spark shell prompt.\n\nYou should understand all potential security problems with running a remote session in browser, so you might want to suspend/destroy app in Marathon when you finish playing with spark-shell.\n\nYou might also notice that it works fine for one browser session, but when two sessions are opened simultaneously it fails with an error like:\nFailed to start database 'metastore_db' with class loader org.apache.spark.sql.hive.client.IsolatedClientLoader$$anon$1@8ddbb68c\n...\nThat happens because gotty starts separate spar-shell process for each connection, and spark-shell creates a metastore_db in a working directory, so second spark-shell instance conflicts with alredy running one.\nTo solve this let’s create a random working directory for each session, following trick worked well for me:\n\"cmd\": \"./gotty -w -p $PORT0 --title-format 'Spark Shell' sh -c 'dir=$RANDOM; mkdir $dir && cd $dir && ../spark-1.5.1-bin-hadoop2.6/bin/spark-shell'\"\nSo a complete example is\n\n\nspark-shell.json\n\n{\n  \"id\": \"/spark-shell\",\n  \"cmd\": \"./gotty -w -p $PORT0 --title-format 'Spark Shell' sh -c 'dir=$RANDOM; mkdir $dir && cd $dir && ../spark-1.5.1-bin-hadoop2.6/bin/spark-shell'\",\n  \"cpus\": 0.2,\n  \"mem\": 256,\n  \"ports\": [\n    0\n  ],\n  \"instances\": 1,\n  \"env\": {\n    \"MESOS_NATIVE_JAVA_LIBRARY\": \"/usr/lib/libmesos.so\",\n    \"SPARK_EXECUTOR_URI\": \"http://d3kbcqa49mib13.cloudfront.net/spark-1.5.1-bin-hadoop2.6.tgz\",\n    \"MASTER\": \"mesos://zk://zk.cluster:2181/mesos\"\n  },\n  \"uris\": [\n    \"http://d3kbcqa49mib13.cloudfront.net/spark-1.5.1-bin-hadoop2.6.tgz\",\n    \"https://github.com/yudai/gotty/releases/download/v0.0.12/gotty_linux_amd64.tar.gz\"\n  ]\n}\n\nThat’s all you need to start playing with spark-shell on Mesos, from here you can tweak settings for your needs, like enabling coarse-grained mode for spark, adding libraries and so on."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Anatoliy Plastinin’s Blog",
    "section": "",
    "text": "Getting started with Terraform locally\n\n\n\n\n\n\nTerraform\n\n\nlocalstack\n\n\nDocker\n\n\n\nQuick demo on how to start playing with terraform using local development environment\n\n\n\n\n\nJul 10, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nUsing Spark SQL and Spark Streaming together\n\n\n\n\n\n\nSpark\n\n\nSpark Streaming\n\n\nSpark SQL\n\n\nKafka\n\n\nDocker\n\n\nJSON\n\n\n\nTutorial on how to get started with Spark SQL, Spark Streaming and Kafka using Docker\n\n\n\n\n\nOct 15, 2017\n\n\n\n\n\n\n\n\n\n\n\n\nspark-shell without Spark\n\n\n\n\n\n\nSpark\n\n\nScala\n\n\n\nSmall trick how to start playing with Spark APIs without having spark distribution installed\n\n\n\n\n\nNov 10, 2016\n\n\n\n\n\n\n\n\n\n\n\n\nWhen Data Driven App Smells Bad\n\n\n\n\n\n\nanti-patterns\n\n\nbig data\n\n\narchitecture\n\n\n\nWhat can go wrong with data driven projects. Lessons learned from failed project.\n\n\n\n\n\nApr 29, 2016\n\n\n\n\n\n\n\n\n\n\n\n\nSpark SQL and Parquet files\n\n\n\n\n\n\nSpark\n\n\nparquet\n\n\n\nTiny note on how to deal with Parquet files with Spark\n\n\n\n\n\nFeb 29, 2016\n\n\n\n\n\n\n\n\n\n\n\n\nProcessing JSON data with Spark SQL\n\n\n\n\n\n\nSpark\n\n\nSpark SQL\n\n\nJSON\n\n\n\nDeep dive into JSON support in Spark SQL\n\n\n\n\n\nJan 30, 2016\n\n\n\n\n\n\n\n\n\n\n\n\nHow to Write Data into Parquet\n\n\n\n\n\n\nparquet\n\n\n\nAn example of how to write data into Apache Parquet format\n\n\n\n\n\nDec 2, 2015\n\n\n\n\n\n\n\n\n\n\n\n\nRunning spark-shell in browser with Apache Mesos and Marathon\n\n\n\n\n\n\nSpark\n\n\nMesos\n\n\nMarathon\n\n\n\nSmall trick on how to run spark-shell a web app using Mesos and Marathon.\n\n\n\n\n\nNov 13, 2015\n\n\n\n\n\n\n\n\n\n\n\n\nGetting started with Spark Streaming using Docker\n\n\n\n\n\n\nSpark\n\n\nKafka\n\n\nDocker\n\n\n\nStep-by-step guide on how to get started with Spark Streaming and Kafka using Docker environment\n\n\n\n\n\nOct 5, 2015\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/2015/10/05/getting-started-with-spark-streaming-using-docker/index.html",
    "href": "blog/2015/10/05/getting-started-with-spark-streaming-using-docker/index.html",
    "title": "Getting started with Spark Streaming using Docker",
    "section": "",
    "text": "Stream processing technologies have been getting a lot of attention lately. You’ve already might heard about Kafka, Spark and its streaming extension.\nIf you’ve always wanted to try Spark Streaming, but never found a time to give it a shot, this post provides you with easy steps on how to get development setup with Spark and Kafka using Docker.\nUpdate: This post is quite outdated, recent version of the tutorial is available here.\nWe will use DirectKafkaWordCount example from spark distribution as basis for our demo. That example shows how to use Spark’s Direct Kafka Stream. You can easily use another example that uses Receiver-based Approach. Discussion of different ways to integrate kafka that spark provides is out of scope of this post. Please checkout kafka integration guide for more details.\nYes, doing another word count demo is boring, but our goal is to learn how to get evrything up and running together and WordCount suits this goal perfectly, rather than learn how to build distributed applications with Spark.\nIf you want just to get code, you can find complete example here."
  },
  {
    "objectID": "blog/2015/10/05/getting-started-with-spark-streaming-using-docker/index.html#the-code",
    "href": "blog/2015/10/05/getting-started-with-spark-streaming-using-docker/index.html#the-code",
    "title": "Getting started with Spark Streaming using Docker",
    "section": "The Code",
    "text": "The Code\nWe will use sbt for building our project. To install it run: brew install sbt.\nNext, we need to setup sbt directory structure for the project. Unfortunately sbt doesn’t provide command to bootstrap a project, so you can create project with your IDE like Eclipse, or use this shell script.\nNow let’s go to code and do some configuration.\nSpark requires packaging all project’s dependencies alongside application, so we will build fat jar that contains app and all dependencies together. We will use sbt assembly plugin for that.\nTo set up this plugin create project/assembly.sbt file with following content:\n\n\nassembly.sbt\n\naddSbtPlugin(\"com.eed3si9n\" % \"sbt-assembly\" % \"0.13.0\")\n\nNow let’s setup our build configuration, build.sbt file should look like:\n\n\nbuild.sbt\n\nname := \"direct_kafka_word_count\"\n\nscalaVersion := \"2.10.5\"\n\nval sparkVersion = \"1.5.1\"\n\nlibraryDependencies ++= Seq(\n  \"org.apache.spark\" %% \"spark-core\" % sparkVersion % \"provided\",\n  \"org.apache.spark\" %% \"spark-streaming\" % sparkVersion % \"provided\",\n  (\"org.apache.spark\" %% \"spark-streaming-kafka\" % sparkVersion) exclude (\"org.spark-project.spark\", \"unused\")\n)\n\nassemblyJarName in assembly := name.value + \".jar\"\n\nAt the moment of writing latest version of spark is 1.5.1 and scala is 2.10.5 for 2.10.x series. Scala 2.10 is used because spark provides pre-built packages for this version only.\nWe don’t need to provide spark libs since they are provided by cluster manager, so those libs are marked as provided.\nThat’s all with build configuration, now let’s write some code. App’s code in src/main/scala/com/example/spark/DirectKafkaWordCount.scala should look like:\n\n\nsrc/main/scala/com/example/spark/DirectKafkaWordCount.scala\n\npackage com.example.spark\n\nimport kafka.serializer.StringDecoder\nimport org.apache.spark.{TaskContext, SparkConf}\nimport org.apache.spark.streaming.kafka.{OffsetRange, HasOffsetRanges, KafkaUtils}\nimport org.apache.spark.streaming.{Seconds, StreamingContext}\n\nobject DirectKafkaWordCount {\n  def main(args: Array[String]): Unit = {\n    if (args.length &lt; 2) {\n      System.err.println(s\"\"\"\n        |Usage: DirectKafkaWordCount &lt;brokers&gt; &lt;topics&gt;\n        |  &lt;brokers&gt; is a list of one or more Kafka brokers\n        |  &lt;topics&gt; is a list of one or more kafka topics to consume from\n        |\n        \"\"\".stripMargin)\n      System.exit(1)\n    }\n\n    val Array(brokers, topics) = args\n\n    // Create context with 10 second batch interval\n    val sparkConf = new SparkConf().setAppName(\"DirectKafkaWordCount\")\n    val ssc = new StreamingContext(sparkConf, Seconds(10))\n\n    // Create direct kafka stream with brokers and topics\n    val topicsSet = topics.split(\",\").toSet\n    val kafkaParams = Map[String, String](\"metadata.broker.list\" -&gt; brokers)\n    val messages = KafkaUtils.createDirectStream[String, String, StringDecoder, StringDecoder](\n      ssc, kafkaParams, topicsSet)\n\n    // Get the lines, split them into words, count the words and print\n    val lines = messages.map(_._2)\n    val words = lines.flatMap(_.split(\" \"))\n    val wordCounts = words.map(x =&gt; (x, 1L)).reduceByKey(_ + _)\n    wordCounts.print()\n\n    // Start the computation\n    ssc.start()\n    ssc.awaitTermination()\n  }\n}\n\nAnd now you can run sbt assembly and find direct_kafka_word_count.jar file in target/scala-2.10 directory.\nThat’s all with coding. Let’s see how to run it."
  },
  {
    "objectID": "blog/2015/10/05/getting-started-with-spark-streaming-using-docker/index.html#installing-docker",
    "href": "blog/2015/10/05/getting-started-with-spark-streaming-using-docker/index.html#installing-docker",
    "title": "Getting started with Spark Streaming using Docker",
    "section": "Installing Docker",
    "text": "Installing Docker\nTo run this example we need two more things: Spark itself and Kafka server.\nAnd that’s where Docker significantly helps us, since we don’t need to install and configure required software, we will simply use Docker images for that.\nEasiest way to get Docker working on OS X is to use docker-machine, which helps to provision Docker on virtual machines.\nTo set it up:\n\nInstall VirtualBox, to run VM with Docker.\nUpdate homebrew to latest version brew update.\nInstall required packages brew install docker docker-machine docker-compose.\n\nAnother option to install everything is to use Docker Toolbox, for more details on how to install toolbox check the official documentation.\nNow we are ready to start Docker VM, run\ndocker-machine create --driver virtualbox --virtualbox-memory 2048 dev\nThis command downloads VM image with Docker host preinstalled, creates a VM named dev with 2Gb of memory and starts it.\nNow you need configure your shell to work with Docker client, just run eval \"$(docker-machine env dev)\". Please note that you need to run this command each time you open a new terminal."
  },
  {
    "objectID": "blog/2015/10/05/getting-started-with-spark-streaming-using-docker/index.html#containers-setup",
    "href": "blog/2015/10/05/getting-started-with-spark-streaming-using-docker/index.html#containers-setup",
    "title": "Getting started with Spark Streaming using Docker",
    "section": "Containers Setup",
    "text": "Containers Setup\nLet’s specify our containers configuration using docker-compose. docker-compose.yml file defines containers and links between them. We will use following configuration:\n\n\ndocker-compose.yml\n\nkafka:\n  image: antlypls/kafka-legacy\n  environment:\n    - KAFKA=localhost:9092\n    - ZOOKEEPER=localhost:2181\n  expose:\n    - \"2181\"\n    - \"9092\"\n\nspark:\n  image: antlypls/spark:1.5.1\n  command: bash\n  volumes:\n    - ./target/scala-2.10:/app\n  links:\n   - kafka\n\nA lot of things are going on here, let’s go through it step by step. This yml file defines two services: kafka and docker.\nkafka service runs image based on spotify/kafka repository, this image provides everything we need for running kafka in one container: kafka broker and zookeeper server.\nAlso two environment variables are added: KAFKA and ZOOKEEPER, those variables are helpful when you run kafka CLI tools inside kafka container, you will see how to do it later.\nWe also expose a kafka broker port 9092 and a port for zookeeper 2181, so linked services can access it.\nThen spark service is defined. I’ve prepared an image antlypls/spark, which provides spark running on YARN. The image is slightly modified version of sequenceiq/spark repository, with spark 1.5.1 and without a few packages that we don’t need for this demo.\nWe specify bash as command, since we want to have interactive shell session within the spark container. volumes option mounts build directory into the spark container, so we will be able to access .jar right in the container. At the end kafka service is linked to spark.\nAnd now we are ready to run everything together.\nLet’s start all containers with docker-compose: docker-compose run --rm spark this starts kafka and then spark and logs us into spark container shell. The --rm flag makes docker-compose to delete corresponding spark container after run.\nBut before running DirectKafkaWordCount app, we need to create a topic in a kafka broker that we are going to read from. Kafka distribution contains a few useful tools to manipulate topics and data: create/list topics, write text messages into a topic and etc. And we can run those tools within kafka container. To do that open a separate terminal session and run:\ndocker exec -it $(docker-compose ps -q kafka) bash\nAnd now let’s create a topic in kafka:\nkafka-topics.sh --create --zookeeper $ZOOKEEPER --replication-factor 1 --partitions 2 --topic word-count\nYou can check that new topic has been created by running commands\n$ kafka-topics.sh --list --zookeeper $ZOOKEEPER\n$ kafka-topics.sh --describe --zookeeper $ZOOKEEPER --topic word-count\nKeep this shell session open, we will use it to add messages to the topic.\nNow go back to spark container shell and run\nspark-submit \\\n--master yarn-client \\\n--class com.example.spark.DirectKafkaWordCount \\\napp/direct_kafka_word_count.jar kafka:9092 word-count\nHere we launch our application in yarn-client mode because we want to see output from the driver.\nYou might see a lot of logs written to output, that is useful for debugging, but it might be hard to see actual app output. You could use following settings for log4j if you wanted to disable those debugging logs:\n\n\nlog4j.properties\n\nlog4j.rootCategory=INFO, console\nlog4j.appender.console=org.apache.log4j.ConsoleAppender\nlog4j.appender.console.target=System.err\nlog4j.appender.console.threshold=ERROR\nlog4j.appender.console.layout=org.apache.log4j.PatternLayout\nlog4j.appender.console.layout.ConversionPattern=%d{yy/MM/dd HH:mm:ss} %p %c{1}: %m%n\n\nReplace $SPARK_HOME/conf/log4j.properties file with one provided above. Or simply put it somewhere in container, e.g. in shared app directory and run app like\nspark-submit \\\n--master yarn-client \\\n--driver-java-options \"-Dlog4j.configuration=file:///app/log4j.properties\" \\\n--class com.example.spark.DirectKafkaWordCount \\\napp/direct_kafka_word_count.jar kafka:9092 word-count\nNote, that docker-compose updates hosts file in running containers, so linked services can be accessed using service name as a hostname. That’s why in our demo we simply use kafka:9090 as a broker address.\nNow let’s add some data into the topic, just run following in the kafka container\nkafka-console-producer.sh --broker-list $KAFKA --topic word-count\nAnd for input like this:\nHello World\n!!!\nYou should see output from spark app like:\n-------------------------------------------\nTime: 1234567890000 ms\n-------------------------------------------\n(Hello,1)\n(World,1)\n(!!!,1)\ndocker-compose doesn’t stop/delete linked containers when run command exits. To stop linked containers run docker-compose stop, and docker-compose rm to delete them.\nAnd that’s it, you have all set up for developing Spark Streaming apps.\nHappy hacking with Spark!"
  },
  {
    "objectID": "blog/2015/12/02/how-to-write-data-into-parquet/index.html",
    "href": "blog/2015/12/02/how-to-write-data-into-parquet/index.html",
    "title": "How to Write Data into Parquet",
    "section": "",
    "text": "Column oriented data stores have proven its success for many analytical purposes. Such success was shown by RCFile and Apache ORC formats and their wide adoption in many distributed data processing tools in Hadoop ecosystem.\nAnother columnar format that has gained popularity lately is Apache Parquet, which is now top level Apache project. It is supported by many data processing tools including Spark and Presto provide support for parquet format.\nRecently I’ve been experimenting with storing data in the parquet format, so I thought it might be a good idea to share a few examples. This post covers the basics of how to write data into parquet."
  },
  {
    "objectID": "blog/2015/12/02/how-to-write-data-into-parquet/index.html#non-hadoop-writer",
    "href": "blog/2015/12/02/how-to-write-data-into-parquet/index.html#non-hadoop-writer",
    "title": "How to Write Data into Parquet",
    "section": "Non-hadoop writer",
    "text": "Non-hadoop writer\nI’m going to show how to implement simple non-hadoop writer. It will be enough to start experimenting with parquet and its options. It also might be useful enough in cases when you have to deal with serial data import and when there is no need for big guns like Spark.\nTo write data in parquet we need to define a schema. And fortunately parquet provides support for popular data serialization libraries, like avro, protocol buffers and thrift. For demo purposes I simply use protobuf.\nPlease, do not be confused, protobuf is a serialization library, but here it’s used only to define record with schema. So we can use ProtoParquetWriter that comes with parquet out-of-the-box. Parquet doesn’t use serialization functionality of any of those libraries, it has its own binary format.\nFrankly, in most cases protobuf is not the best choice for defining record schema, since it doesn’t has many types that parquet provides, like DECIMAL or INT96 for timestamps.\nIf you are interested in low level details of how to write custom data classes checkout following examples and benchmarks from parquet repository.\nSuppose we have an Event class generated from protobuf definition, so we can write collection of events like this\nval compressionCodecName = CompressionCodecName.GZIP\nval blockSize = 256 * 1024 * 1024\nval pageSize = 1 * 1024 * 1024\nval outputPath = new Path(\"data.parquet\")\n\nval parquetWriter = new ProtoParquetWriter[Event](outputPath, classOf[Event], compressionCodecName, blockSize, pageSize)\n\nevents.foreach(parquetWriter.write)\nparquetWriter.close()\nPage size is an amount of data buffered before it’s written as page. This setting might affect compression performance. Block (row group) size is an amount of data buffered in memory before it is written to disc. For more details about what pages and row groups are, please see parquet format documentation."
  },
  {
    "objectID": "blog/2015/12/02/how-to-write-data-into-parquet/index.html#writing-parquet-files-to-s3",
    "href": "blog/2015/12/02/how-to-write-data-into-parquet/index.html#writing-parquet-files-to-s3",
    "title": "How to Write Data into Parquet",
    "section": "Writing parquet files to S3",
    "text": "Writing parquet files to S3\nNow let’s see how to write parquet files directly to Amazon S3. This can be done using Hadoop S3 file systems. More precisely, here we’ll use S3A file system. It will require a few code changes, we’ll use ParquetWriter class to be able to pass conf object with AWS settings.\nval compressionCodecName = CompressionCodecName.GZIP\nval blockSize = 256 * 1024 * 1024\nval pageSize = 1 * 1024 * 1024\nval outputPath = new Path(\"s3a://bucket/data.parquet\")\n\nval accessKey = \"aws_access_key\"\nval secretKey = \"aws_secret_key\"\nval conf = new Configuration\nconf.set(\"fs.s3a.access.key\", accessKey)\nconf.set(\"fs.s3a.secret.key\", secretKey)\n\nval writeSupport = new ProtoWriteSupport[Event](classOf[Event])\n\nval parquetWriter = new ParquetWriter[Event](outputPath,\n  writeSupport, compressionCodecName, blockSize, pageSize, pageSize,\n  ParquetWriter.DEFAULT_IS_DICTIONARY_ENABLED, ParquetWriter.DEFAULT_IS_VALIDATING_ENABLED,\n  ParquetWriter.DEFAULT_WRITER_VERSION,\n  conf\n)\nNow you should have enough information to start experimenting with parquet, you can find examples from this post as a single project in this repository."
  },
  {
    "objectID": "blog/2017/10/15/using-spark-sql-and-spark-streaming-together/index.html",
    "href": "blog/2017/10/15/using-spark-sql-and-spark-streaming-together/index.html",
    "title": "Using Spark SQL and Spark Streaming together",
    "section": "",
    "text": "It’s been 2 years since I wrote first tutorial on how to setup local docker environment for running Spark Streaming jobs with Kafka. This post is the follow-up to the previous one, but a little bit more advanced and up to date. It shows basic working example of Spark application that uses Spark SQL to process data stream from Kafka. I’ll also show how to run Spark application and setup local development environment with all components (ZooKeepr, Kafka) using docker and docker-compose.\nTL;DR Check out this repository if you just want to see the code of the complete example."
  },
  {
    "objectID": "blog/2017/10/15/using-spark-sql-and-spark-streaming-together/index.html#setting-up-project-with-sbt",
    "href": "blog/2017/10/15/using-spark-sql-and-spark-streaming-together/index.html#setting-up-project-with-sbt",
    "title": "Using Spark SQL and Spark Streaming together",
    "section": "Setting up Project with sbt",
    "text": "Setting up Project with sbt\nSpark requires packaging all project’s dependencies alongside application, so we will build fat jar that contains app and all its dependencies together. In this tutorial we’ll use sbt with sbt-assembly plugin to build a fat jar with our demo app.\nTo add sbt-assembly to the project, create project/plugins.sbt file:\n\n\nproject/plugins.sbt\n\naddSbtPlugin(\"com.eed3si9n\" % \"sbt-assembly\" % \"0.14.5\")\n\nNow let’s define build configuration in build.sbt file:\n\n\nbuild.sbt\n\nname := \"kafka-spark-demo\"\n\nscalaVersion := \"2.11.11\"\n\nval sparkVersion = \"2.2.0\"\n\nlibraryDependencies ++= Seq(\n  \"org.apache.spark\" %% \"spark-core\" % sparkVersion % \"provided\",\n  \"org.apache.spark\" %% \"spark-sql\" % sparkVersion % \"provided\",\n  \"org.apache.spark\" %% \"spark-streaming\" % sparkVersion % \"provided\",\n  \"org.apache.spark\" %% \"spark-streaming-kafka-0-10\" % sparkVersion excludeAll(\n    ExclusionRule(organization = \"org.spark-project.spark\", name = \"unused\"),\n    ExclusionRule(organization = \"org.apache.spark\", name = \"spark-streaming\"),\n    ExclusionRule(organization = \"org.apache.hadoop\")\n  )\n)\n\ntarget in assembly := file(\"build\")\n\nassemblyJarName in assembly := s\"${name.value}.jar\"\n\nA few things are going there. First, we define versions of Scala and Spark.\nNext, we define dependencies. spark-core, spark-sql and spark-streaming are marked as provided because they are already included in the spark distribution. Also a few exclusion rules are specified for spark-streaming-kafka-0-10 in order to exclude transitive dependencies that lead to assembly merge conflicts.\nFinally we override the fat jar file name and the target directory where the fat jar is saved."
  },
  {
    "objectID": "blog/2017/10/15/using-spark-sql-and-spark-streaming-together/index.html#spark-streaming-application",
    "href": "blog/2017/10/15/using-spark-sql-and-spark-streaming-together/index.html#spark-streaming-application",
    "title": "Using Spark SQL and Spark Streaming together",
    "section": "Spark Streaming Application",
    "text": "Spark Streaming Application\nAs an example we’ll write simple application that processes json data from Kafka using Spark SQL. App will compute number of different actions in a stream of JSON events like this:\n{\"action\":\"update\",\"timestamp\":\"2017-10-05T23:02:51Z\"}\nNow let’s jump into the code, I will walk through the steps for making Spark Streaming and Spark SQL work together.\n\n\nsrc/main/scala/com/antlypls/blog/KafkaSparkDemo.scala\n\npackage com.antlypls.blog\n\nimport org.apache.kafka.common.serialization.StringDeserializer\nimport org.apache.spark.sql.SparkSession\nimport org.apache.spark.sql.functions.count\nimport org.apache.spark.sql.types.{StringType, StructType, TimestampType}\nimport org.apache.spark.streaming.kafka010.ConsumerStrategies.Subscribe\nimport org.apache.spark.streaming.kafka010.KafkaUtils\nimport org.apache.spark.streaming.kafka010.LocationStrategies.PreferConsistent\nimport org.apache.spark.streaming.{Seconds, StreamingContext}\n\nobject KafkaSparkDemo {\n  def main(args: Array[String]): Unit = {\n    // Configurations for kafka consumer\n    val kafkaBrokers = sys.env.get(\"KAFKA_BROKERS\")\n    val kafkaGroupId = sys.env.get(\"KAFKA_GROUP_ID\")\n    val kafkaTopic = sys.env.get(\"KAFKA_TOPIC\")\n\n    // Verify that all settings are set\n    require(kafkaBrokers.isDefined, \"KAFKA_BROKERS has not been set\")\n    require(kafkaGroupId.isDefined, \"KAFKA_GROUP_ID has not been set\")\n    require(kafkaTopic.isDefined, \"KAFKA_TOPIC has not been set\")\n\n    // Create Spark Session\n    val spark = SparkSession\n      .builder()\n      .appName(\"KafkaSparkDemo\")\n      .getOrCreate()\n\n    import spark.implicits._\n\n    // Create Streaming Context and Kafka Direct Stream with provided settings and 10 seconds batches\n    val ssc = new StreamingContext(spark.sparkContext, Seconds(10))\n\n    val kafkaParams = Map[String, Object](\n      \"bootstrap.servers\" -&gt; kafkaBrokers.get,\n      \"key.deserializer\" -&gt; classOf[StringDeserializer],\n      \"value.deserializer\" -&gt; classOf[StringDeserializer],\n      \"group.id\" -&gt; kafkaGroupId.get,\n      \"auto.offset.reset\" -&gt; \"latest\"\n    )\n\n    val topics = Array(kafkaTopic.get)\n    val stream = KafkaUtils.createDirectStream[String, String](\n      ssc,\n      PreferConsistent,\n      Subscribe[String, String](topics, kafkaParams)\n    )\n\n    // Define a schema for JSON data\n    val schema = new StructType()\n      .add(\"action\", StringType)\n      .add(\"timestamp\", TimestampType)\n\n    // Process batches:\n    // Parse JSON and create Data Frame\n    // Execute computation on that Data Frame and print result\n    stream.foreachRDD { (rdd, time) =&gt;\n      val data = rdd.map(record =&gt; record.value)\n      val json = spark.read.schema(schema).json(data)\n      val result = json.groupBy($\"action\").agg(count(\"*\").alias(\"count\"))\n      result.show\n    }\n\n    // Start Stream\n    ssc.start()\n    ssc.awaitTermination()\n  }\n}\n\nApplication requires following parameters to be defined via environment variables:\n\nKAFKA_BROKERS — list of Kafka brokers used for initial discovery in the form host1:port1,host2:port2,...;\nKAFKA_GROUP_ID — unique string that identifies the consumer group;\nKAFKA_TOPIC — name of the topic to consume data from.\n\nNow you can build fat jar by running sbt assembly from project’s root. After that you should find kafka-spark-demo.jar in the build directory."
  },
  {
    "objectID": "blog/2017/10/15/using-spark-sql-and-spark-streaming-together/index.html#containers-setup-with-docker-compose",
    "href": "blog/2017/10/15/using-spark-sql-and-spark-streaming-together/index.html#containers-setup-with-docker-compose",
    "title": "Using Spark SQL and Spark Streaming together",
    "section": "Containers setup with docker-compose",
    "text": "Containers setup with docker-compose\nIn order to run our example we need three things:\n\ncontainer with java where we’ll run our app;\ncontainer with Kafka;\nand container with ZooKeeper, required by Kafka.\n\nSo let’s define all three components with docker-compose.\n\n\ndocker-compose.yml\n\nversion: '3'\n\nservices:\n  zookeeper:\n    image: antlypls/zookeeper\n\n  kafka:\n    image: antlypls/kafka\n    depends_on:\n      - zookeeper\n    environment:\n      KAFKA_CREATE_TOPICS: \"events:1:1\"\n      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181\n\n  java:\n    image: openjdk:jre\n    command: bash\n    ports:\n      - \"4040:4040\"\n    volumes:\n      - ./build:/build\n    working_dir: /build\n    depends_on:\n      - zookeeper\n      - kafka\n\nPlease note that zookeeper image exposes port 2181 and kafka image exposes port 9092, so those ports will be available inside containers.\nAlso kafka image provides easy way to create topic at startup time via KAFKA_CREATE_TOPICS environment variable. In this example events topic with 1 partition and replication factor 1 will be created. KAFKA_ZOOKEEPER_CONNECT variable defines a connection string for the zookeeper connection. For more details about this containers see their source code (ZooKeeper, Kafka).\nIn the end docker-compose.yml defines java service based on openjdk:jre image. We’ll use this container to run our Spark application. That’s why build directory with fat jar is mounted into container. Also Spark Web UI port 4040 is forwarded to the host machine as well, so you’ll be able to open Web UI at runtime and see various stats and information about Spark execution.\nNow we miss only one part: Spark distribution itself. For this demo download it into build directory, for example you can do it with wget:\nwget https://d3kbcqa49mib13.cloudfront.net/spark-2.2.0-bin-hadoop2.7.tgz\nAnd then unpack it tar -xzf spark-2.2.0-bin-hadoop2.7.tgz. Now you should have spark-2.2.0-bin-hadoop2.7 directory inside build. At this point we don’t need the distribution archive anymore, so you can delete it."
  },
  {
    "objectID": "blog/2017/10/15/using-spark-sql-and-spark-streaming-together/index.html#running-demo-application",
    "href": "blog/2017/10/15/using-spark-sql-and-spark-streaming-together/index.html#running-demo-application",
    "title": "Using Spark SQL and Spark Streaming together",
    "section": "Running Demo Application",
    "text": "Running Demo Application\nAnd now we’re finally all set to run the whole thing.\nJust run from project’s root docker-compose run --rm --service-ports java. Here --rm flag makes docker-compose to delete corresponding spark container after run, and --service-ports flag publishes services’ ports to host.\nNow you should be logged in to the java container’s shell and have working directory set to /build.\nHere we’ll just run demo app in a local mode, I don’t want to dive deep into how to run spark applications on a cluster with docker, as it deserves separate long blog post of its own.\nIn the java container terminal run the following:\nKAFKA_BROKERS=kafka:9092 \\\nKAFKA_GROUP_ID=spark-streaming-demo \\\nKAFKA_TOPIC=events \\\nspark-2.2.0-bin-hadoop2.7/bin/spark-submit \\\n  --master local[*] \\\n  --class com.antlypls.blog.KafkaSparkDemo kafka-spark-demo.jar\nYou should see a lot of debug output from Spark, and in between all that debug noise you should see:\n+------+-----+\n|action|count|\n+------+-----+\n+------+-----+\nAll batches are empty so far, since we haven’t sent any data to the Kafka topic. So let’s write some data into kafka topic, run this command\ndocker exec -it $(docker-compose ps -q kafka) kafka-console-producer.sh --broker-list localhost:9092 --topic events\nAnd paste into terminal messages like this:\n{\"action\":\"create\",\"timestamp\":\"2017-10-05T23:01:17Z\"}\n{\"action\":\"update\",\"timestamp\":\"2017-10-05T23:01:19Z\"}\n{\"action\":\"update\",\"timestamp\":\"2017-10-05T23:02:51Z\"}\nAfter that you should see output like:\n+------+-----+\n|action|count|\n+------+-----+\n|create|    1|\n|update|    2|\n+------+-----+"
  },
  {
    "objectID": "blog/2017/10/15/using-spark-sql-and-spark-streaming-together/index.html#final-notes",
    "href": "blog/2017/10/15/using-spark-sql-and-spark-streaming-together/index.html#final-notes",
    "title": "Using Spark SQL and Spark Streaming together",
    "section": "Final notes",
    "text": "Final notes\nIt might be hard to see actual app’s output because of huge amounts of debug information. In order to set log level to ERROR put following log4j.properties file in spark-2.2.0-bin-hadoop2.7/conf directory:\n\n\nspark-2.2.0-bin-hadoop2.7/conf/log4j.properties\n\nlog4j.rootCategory=INFO, console\nlog4j.appender.console=org.apache.log4j.ConsoleAppender\nlog4j.appender.console.target=System.err\nlog4j.appender.console.threshold=ERROR\nlog4j.appender.console.layout=org.apache.log4j.PatternLayout\nlog4j.appender.console.layout.ConversionPattern=%d{yy/MM/dd HH:mm:ss} %p %c{1}: %m%n\n\ndocker-compose doesn’t stop/delete linked containers when run command exits. So after you exited from java service session, you might want to run docker-compose stop and docker-compose rm to stop and delete zookeeper and kafka containers.\nIt’s also possible to implement same functionality using Structured Streaming, but that is going to have to wait for another post.\nCheck out this post if you want to learn more about JSON data processing with Spark.\nAnd that’s basically all you need to know to start developing Spark Streaming applications and run/test them locally with docker.\nI hope that post was helpful!"
  },
  {
    "objectID": "blog/2020/07/10/terraform-localstack-demo/index.html",
    "href": "blog/2020/07/10/terraform-localstack-demo/index.html",
    "title": "Getting started with Terraform locally",
    "section": "",
    "text": "Note: This post was originally written in July 2020, and later updated and published in June 2024."
  },
  {
    "objectID": "blog/2020/07/10/terraform-localstack-demo/index.html#introduction",
    "href": "blog/2020/07/10/terraform-localstack-demo/index.html#introduction",
    "title": "Getting started with Terraform locally",
    "section": "Introduction",
    "text": "Introduction\nThis post is inspired by a workshop I made for a team of data scientists who owned a bunch of large datasets stored in an S3 bucket, and they managed S3 only via the web console. It worked fine when they only needed to add a bucket once in a while. But the real challenge came when the team had to reduce costs. They had to update the storage class of their data to “Infrequent Access”, as well as add lifecycle rules to automate this process, ensuring that unused data is moved to a more cost-effective storage class.\nI was surprised that there was no declarative approach in place. Manual changes via the web console have multiple issues: it is mindeless time-consuming clicking around, and it’s prone to errors, very easy to simply forget something, or make a typo. It’s hard to organize peer review for such an approach, unless multiple people sit and click together.\nSo having Terraform would have helped to deal with those problems, though the team didn’t have any experience with tools like that. To address this gap, I created a quick demo to show how Terraform can be used to manage S3 buckets.\nIf you’ve read my other posts, you may have noticed that I’m a huge fan of reproducible setups and research, using tools like Docker Compose. So I used it for this demo too, together with localstack, a tool that emulates the AWS environment, to avoid the need for a real AWS account. This makes the demo entirely local, enabling any team member to play with it and see how easy it is to use Terraform."
  },
  {
    "objectID": "blog/2020/07/10/terraform-localstack-demo/index.html#the-setup",
    "href": "blog/2020/07/10/terraform-localstack-demo/index.html#the-setup",
    "title": "Getting started with Terraform locally",
    "section": "The Setup",
    "text": "The Setup\nOk, after such a long intro, let’s move to the actual demo. If you’re eager to try it immediately, check out this repository.\n\nNote: Here I assume that you already have Docker (or any other similar tool, like Podman) installed.\n\nFor the purpose of the demo, we’ll need two containers: one that runs localstack, and the other one for running terraform commands, and other tools, like aws, if needed.\nHere is how we can do it.\n\n\ndocker-compose.yaml\n\nservices:\n  localstack:\n    image: localstack/localstack\n    ports:\n      - \"4566:4566\"\n\n  terraform:\n    build: .\n    depends_on:\n      - localstack\n    stdin_open: true\n    tty: true\n    volumes:\n      - ./terraform:/terraform\n    working_dir: /terraform\n\nThe localstack configuration is pretty simple, we just use the official docker image and set port mappings, in case you want to access the service from the outside.\nThe terraform definition is slightly more complex, but still straightforward. First, we use a build parameter to use a custom Dockerfile to build the container, so we can customize it and install extra tools there, we’ll discuss it later.\nNext, we have a depends_on option that says that the terraform container should start after localstack. It’s not strictly needed for this setup, but we have it here for clarity. The next two settings allow us to use the container to run command-line tools.\nThe volume parameter is used to map our local terraform directory into the container, we’ll add terraform files in there.\nLastly, we set the working directory (working_dir) to point to the mapped one.\nNow, let’s take a quick look at the Dockerfile we use for the terraform service.\n\n\nDockerfile\n\nFROM hashicorp/terraform:latest\n\nRUN apk add --no-cache aws-cli\n\nENTRYPOINT [\"sh\"]\n\nIt’s quite straightforward as we use a pre-built container for terraform.\nFor demonstration purposes, we add the AWS command-line tool, which might be useful for testing. And also we redefine the entrypoint to use shell, because the default one is set to terraform."
  },
  {
    "objectID": "blog/2020/07/10/terraform-localstack-demo/index.html#running-the-demo",
    "href": "blog/2020/07/10/terraform-localstack-demo/index.html#running-the-demo",
    "title": "Getting started with Terraform locally",
    "section": "Running the demo",
    "text": "Running the demo\nNow let’s see how we can use this configuration.\nFirst, we need to build a terraform container because we use a custom Dockerfile. To do this, we use the docker-compose build command.\nOnce the build is complete, we can launch our setup with docker-compose up.\nAnd after, you should see something like\nlocalstack-1  | LocalStack build date: 2024-06-03\nlocalstack-1  | LocalStack build git hash: 5085b532c\n...\nlocalstack-1  | Ready.\nmeaning that localstack has successfully started. You may notice a few errors in the log, but they’re not relevant for our setup.\nNow we can connect to the terraform container by running docker-compose exec terraform sh in a separate terminal. Once you are in the helper container shell, you will be in the mapped local terraform directory, because we set the working directory in the docker-compose file.\nYou can check that you actually have terraform here by running terraform --version. If you did everything correctly, you should see something like\n/terraform # terraform --version\nTerraform v1.8.5\non linux_arm64\n/terraform #\nHere we can see the Terraform command is available for us. And we have version 1.8.5, the latest at the time of writing.\nNext, let’s look at how to configure terraform to work with localstack.\nLet’s add a main.tf file to the terraform directory like:\n\n\nmain.tf\n\nterraform {\n  required_providers {\n    aws = {\n      source  = \"hashicorp/aws\"\n      version = \"~&gt; 5.54.1\"\n    }\n  }\n\n  backend \"local\" {\n    path = \"terraform.tfstate\"\n  }\n}\n\nprovider \"aws\" {\n  access_key = \"test\"\n  secret_key = \"test\"\n  region = \"us-east-1\"\n\n  s3_use_path_style = true\n  endpoints {\n    s3  = \"http://localstack:4566\"\n    sts = \"http://localstack:4566\"\n  }\n}\n\nThis is a pretty standard configuration for AWS. First, we require the AWS provider, needed to interact with AWS, you may want to update it to a more recent version. Also, we store the terraform state as a local file, which is enough for local development purposes.\nIn the next block, we configure the AWS provider itself. We set credentials as just test for the localstack. And then we set API endpoints to point to the localstack ones. We set the S3 endpoint, since that’s what we’ll use for the demo, and STS, just because by default terraform tries to validate credentials via the AWS STS service.\nAnd now we’re all set to see our setup in action. Go back to the terminal, and run terraform init to initialize terraform. This command might take a little time because it pulls the necessary dependencies. When it’s done, you should see:\n/terraform # terraform init\n\nInitializing the backend...\n\nSuccessfully configured the backend \"local\"! Terraform will automatically\nuse this backend unless the backend configuration changes.\n\nInitializing provider plugins...\n- Finding hashicorp/aws versions matching \"~&gt; 5.54.1\"...\n- Installing hashicorp/aws v5.54.1...\n- Installed hashicorp/aws v5.54.1 (signed by HashiCorp)\n\nTerraform has created a lock file .terraform.lock.hcl to record the provider\nselections it made above. Include this file in your version control repository\nso that Terraform can guarantee to make the same selections by default when\nyou run \"terraform init\" in the future.\n\nTerraform has been successfully initialized!\nAfter that, you can notice that there are some new files in the terraform directory. The lock file ensures consistent provider versions, and the .terraform directory contains all downloaded plugins. We don’t really need to modify or do anything with those files.\nNow, let’s run terraform plan, you should see:\n/terraform # terraform plan\n\nNo changes. Your infrastructure matches the configuration.\n\nTerraform has compared your real infrastructure against your configuration and found no differences, so no changes are needed.\nAs expected, it shows no changes because we haven’t set up any infrastructure yet. Let’s change that by adding an S3 bucket.\nJust create a new file buckets.tf.\n\n\nbuckets.tf\n\nresource \"aws_s3_bucket\" \"bucket_demo\" {\n  bucket = \"demo\"\n}\n\nNow go back to the terminal and run terraform plan again:\n/terraform # terraform plan\n\nTerraform used the selected providers to generate the following execution plan. Resource actions are indicated with the following symbols:\n  + create\n\nTerraform will perform the following actions:\n\n  # aws_s3_bucket.bucket_demo will be created\n  + resource \"aws_s3_bucket\" \"bucket_demo\" {\n      + acceleration_status         = (known after apply)\n      + acl                         = (known after apply)\n      + arn                         = (known after apply)\n      + bucket                      = \"demo\"\n      + bucket_domain_name          = (known after apply)\n      + bucket_prefix               = (known after apply)\n      + bucket_regional_domain_name = (known after apply)\n      + force_destroy               = false\n      + hosted_zone_id              = (known after apply)\n      + id                          = (known after apply)\n      + object_lock_enabled         = (known after apply)\n      + policy                      = (known after apply)\n      + region                      = (known after apply)\n      + request_payer               = (known after apply)\n      + tags_all                    = (known after apply)\n      + website_domain              = (known after apply)\n      + website_endpoint            = (known after apply)\n    }\n\nPlan: 1 to add, 0 to change, 0 to destroy.\nNow we can see the difference. Terraform says that it plans to add one resource, which is our bucket. But to actually create the new bucket, we need to execute terraform apply. The apply command checks the state, shows the plan too, and asks if you want to continue, type yes.\n/terraform # terraform apply\n\n...\n\nDo you want to perform these actions?\n  Terraform will perform the actions described above.\n  Only 'yes' will be accepted to approve.\n\n  Enter a value: yes\n\naws_s3_bucket.bucket_demo: Creating...\naws_s3_bucket.bucket_demo: Creation complete after 1s [id=demo]\n\nApply complete! Resources: 1 added, 0 changed, 0 destroyed.\nApply complete! confirms that changes are applied. And you can double-check it by running terraform plan again.\nNow you can see that there is a new file called terraform.tfstate. This is the file that we set up earlier in a local backend to keep the state.\nAlso, to showcase the aws tool we installed via Dockerfile, let’s check that the bucket was created using the aws command.\nFirst, let’s run aws configure, and you can use test as credentials here too.\n/terraform # aws configure\nAWS Access Key ID [None]: test\nAWS Secret Access Key [None]: test\nDefault region name [None]:\nDefault output format [None]:\nBut configure didn’t ask us for endpoint_url that we need to override and point to localstack. We can set it explicitly by running aws configure set endpoint_url http://localstack:4566.\nNow you can run aws s3 ls, and you’ll see demo in the output.\n/terraform # aws s3 ls\n2024-06-15 00:00:01 demo\nAnd that’s it! As you can see, it is quite simple with just a few files and containers help, you have an environment to play with terraform and see how it can be used to manage AWS infrastructure without actually using AWS or installing any tools on your machine, besides Docker itself.\nThis demo is completely reproducible. You can take this template and experiment with terraform on your own."
  },
  {
    "objectID": "blog/2016/02/29/spark-sql-and-parquet-files/index.html",
    "href": "blog/2016/02/29/spark-sql-and-parquet-files/index.html",
    "title": "Spark SQL and Parquet files",
    "section": "",
    "text": "This is a short note on how to deal with Parquet files with Spark.\n\nPreviously I showed how to write parquet files using just parquet library.\nBut Spark SQL has built-in support for Parquet data format, which makes processing data in parquet files easy using simple DataFrames API.\nReading DataFrame from parquet is simple as:\nval df = sqlContext.read.parquet(\"s3a://bucket/data/\")\nAnd writing data to parquet files:\ndf.write.parquet(\"s3a://bucket/another_data/\")\nMore advanced topics like partitioning and schema merging will be covered later."
  },
  {
    "objectID": "blog/2016/01/30/processing-json-data-with-sparksql/index.html",
    "href": "blog/2016/01/30/processing-json-data-with-sparksql/index.html",
    "title": "Processing JSON data with Spark SQL",
    "section": "",
    "text": "Spark SQL provides built-in support for variety of data formats, including JSON. Each new release of Spark contains enhancements that make use of DataFrames API with JSON data more convenient. Same time, there are a number of tricky aspects that might lead to unexpected results. In this post I’ll show how to use Spark SQL to deal with JSON.\nJSON is very simple, human-readable and easy to use format. But its simplicity can lead to problems, since it’s schema-less. Especially when you have to deal with unreliable third-party data sources, such services may return crazy JSON responses containing integer numbers as strings, or encode nulls different ways like null, \"\" or even \"null\"."
  },
  {
    "objectID": "blog/2016/01/30/processing-json-data-with-sparksql/index.html#loading-data",
    "href": "blog/2016/01/30/processing-json-data-with-sparksql/index.html#loading-data",
    "title": "Processing JSON data with Spark SQL",
    "section": "Loading data",
    "text": "Loading data\nYou can read and parse JSON to DataFrame directly from file:\nval df = sqlContext.read.json(\"s3a://some-bucket/some-file.json\")\nPlease note Spark expects each line to be a separate JSON object, so it will fail if you’ll try to load a pretty formatted JSON file.\nAlso you read JSON data from RDD[String] object like:\n// construct RDD[Sting]\nval events = sc.parallelize(\n  \"\"\"{\"action\":\"create\",\"timestamp\":\"2016-01-07T00:01:17Z\"}\"\"\" :: Nil)\n\n// read it\nval df = sqlContext.read.json(events)\nThe latter option is also useful for reading JSON messages with Spark Streaming. Check out this post for example of how to process JSON data from Kafka using Spark Streaming.\nIf you are just playing around with DataFrames you can use show method to print DataFrame to console.\nscala&gt; df.show\n+------+--------------------+\n|action|           timestamp|\n+------+--------------------+\n|create|2016-01-07T00:01:17Z|\n+------+--------------------+"
  },
  {
    "objectID": "blog/2016/01/30/processing-json-data-with-sparksql/index.html#schema-inference-and-explicit-definition",
    "href": "blog/2016/01/30/processing-json-data-with-sparksql/index.html#schema-inference-and-explicit-definition",
    "title": "Processing JSON data with Spark SQL",
    "section": "Schema inference and explicit definition",
    "text": "Schema inference and explicit definition\nSimply running sqlContext.read.json(events) will not load data, since DataFrames are evaluated lazily. But it will trigger schema inference, spark will go over RDD to determine schema that fits the data.\nIn the shell you can print schema using printSchema method:\nscala&gt; df.printSchema\nroot\n  |-- action: string (nullable = true)\n  |-- timestamp: string (nullable = true)\nAs you saw in the last example Spark inferred type of both columns as strings.\nIt is possible to provide schema explicitly to avoid that extra scan:\nval schema = (new StructType).add(\"action\", StringType).add(\"timestamp\", TimestampType)\n\nval df = sqlContext.read.schema(schema).json(events)\n\ndf.show\n\n// +------+--------------------+\n// |action|           timestamp|\n// +------+--------------------+\n// |create|2016-01-07 01:01:...|\n// +------+--------------------+\nAs you might have noticed type of timestamp column is explicitly forced to be a TimestampType. It’s important to understand that this type coercion is performed in JSON parser, and it has nothing to do with DataFrame’s type casting functionality. Type coercions implemented in parser are somewhat limited and in some cases unobvious. Following example demonstrates it:\nval events = sc.parallelize(\n  \"\"\"{\"action\":\"create\",\"timestamp\":1452121277}\"\"\" ::\n  \"\"\"{\"action\":\"create\",\"timestamp\":\"1452121277\"}\"\"\" ::\n  \"\"\"{\"action\":\"create\",\"timestamp\":\"\"}\"\"\" ::\n  \"\"\"{\"action\":\"create\",\"timestamp\":null}\"\"\" ::\n  \"\"\"{\"action\":\"create\",\"timestamp\":\"null\"}\"\"\" ::\n  Nil\n)\n\nval schema = (new StructType).add(\"action\", StringType).add(\"timestamp\", LongType)\n\nsqlContext.read.schema(schema).json(events).show\n\n// +------+----------+\n// |action| timestamp|\n// +------+----------+\n// |create|1452121277|\n// |  null|      null|\n// |create|      null|\n// |create|      null|\n// |  null|      null|\n// +------+----------+\nFrankly that is not a result that one can expect. Look at 2nd row in the result set, as you may see, there is no conversion from string to integer. But here is one more big problem, if you try to set type for which parser doesn’t has conversion, it won’t simply discard value and set that field to null, instead it will consider entire row as incorrect, and set all fields to nulls. The good news is that you can read all values as strings."
  },
  {
    "objectID": "blog/2016/01/30/processing-json-data-with-sparksql/index.html#type-casting",
    "href": "blog/2016/01/30/processing-json-data-with-sparksql/index.html#type-casting",
    "title": "Processing JSON data with Spark SQL",
    "section": "Type casting",
    "text": "Type casting\nIf you can’t be sure in a quality of you data, the best option is to explicitly provide schema forcing StringType for all untrusted fields to avoid extra RDD scan, and then cast those columns to desired type:\nval schema = (new StructType).add(\"action\", StringType).add(\"timestamp\", StringType)\n\nsqlContext.read.schema(schema).json(events).select($\"action\", $\"timestamp\".cast(LongType)).show\n\n// +------+----------+\n// |action| timestamp|\n// +------+----------+\n// |create|1452121277|\n// |create|1452121277|\n// |create|      null|\n// |create|      null|\n// |create|      null|\n// +------+----------+\nNow that’s more like a sane result.\nSpark’s catalyst optimizer has a very powerful type casting functionality, let’s see how we can parse UNIX timestamps from the previous example:\nval schema = (new StructType).add(\"action\", StringType).add(\"timestamp\", StringType)\n\nsqlContext.read.schema(schema).json(events)\n  .select($\"action\", $\"timestamp\".cast(LongType).cast(TimestampType))\n  .show\n\n// +------+--------------------+\n// |action|           timestamp|\n// +------+--------------------+\n// |create|2016-01-07 00:01:...|\n// |create|2016-01-07 00:01:...|\n// |create|                null|\n// |create|                null|\n// |create|                null|\n// +------+--------------------+\nSpark allows to parse integer timestamps as a timestamp type, but right now (as of spark 1.6) there exists a difference in behavior: parser treats integer value as a number of milliseconds, but catalysts cast behavior is treat as a number of seconds. This behavior is about to change in Spark 2.0 (see SPARK-12744)."
  },
  {
    "objectID": "blog/2016/01/30/processing-json-data-with-sparksql/index.html#handling-nested-objects",
    "href": "blog/2016/01/30/processing-json-data-with-sparksql/index.html#handling-nested-objects",
    "title": "Processing JSON data with Spark SQL",
    "section": "Handling nested objects",
    "text": "Handling nested objects\nOften in API responses useful data might be wrapped in a several layers of nested objects:\n{\n  \"payload\": {\n    \"event\": {\n      \"action\": \"create\",\n      \"timestamp\": 1452121277\n    }\n  }\n}\nStar (*) expansion makes it easier to unnest with such objects, for example:\nval vals = sc.parallelize(\n  \"\"\"{\"payload\":{\"event\":{\"action\":\"create\",\"timestamp\":1452121277}}}\"\"\" ::\n  Nil\n)\n\nval schema = (new StructType)\n  .add(\"payload\", (new StructType)\n    .add(\"event\", (new StructType)\n      .add(\"action\", StringType)\n      .add(\"timestamp\", LongType)\n    )\n  )\n\nsqlContext.read.schema(schema).json(vals).select($\"payload.event.*\").show\n\n// +------+----------+\n// |action| timestamp|\n// +------+----------+\n// |create|1452121277|\n// +------+----------+\nIf you need more control over column names, you can always use as method to rename columns, e.g.:\nsqlContext.read.schema(schema).json(vals)\n  .select(\n    $\"payload.event.action\".as(\"event_action\"),\n    $\"payload.event.timestamp\".as(\"event_timestamp\")\n  ).show\n\n// +------------+---------------+\n// |event_action|event_timestamp|\n// +------------+---------------+\n// |      create|     1452121277|\n// +------------+---------------+"
  },
  {
    "objectID": "blog/2016/01/30/processing-json-data-with-sparksql/index.html#wrapping-up",
    "href": "blog/2016/01/30/processing-json-data-with-sparksql/index.html#wrapping-up",
    "title": "Processing JSON data with Spark SQL",
    "section": "Wrapping up",
    "text": "Wrapping up\nThat were quite a few tricks and things to keep in mind when dealing with JSON data. In conclusion I’d like to say obvious thing — do not disregard unit tests for data input and data transformations, especially when you have no control over data source. Also it will make you more confident doing upgrade to newer version of Spark, since parsing and casting behavior might change in the future."
  }
]